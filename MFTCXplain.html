<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>MFTCXplain – Yalda Daryani</title>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; max-width: 1200px; margin: 0 auto; padding: 0; color: #333; }
    .navbar { background-color: #f8f8f8; padding: 15px 0; text-align: center; box-shadow: 0 2px 4px rgba(0,0,0,0.1); margin-bottom: 30px; width: 100%; }
    .navbar a { color: #333; text-decoration: none; margin: 0 15px; font-weight: 500; font-size: 16px; transition: color 0.3s; }
    .navbar a:hover { color: #0366d6; }
    .content { padding: 0 20px; }
    .project-header { margin-bottom: 40px; text-align: center; }
    .project-title { font-size: 2.5em; margin-bottom: 10px; color: #333; }
    .project-subtitle { font-size: 1.2em; color: #666; margin-bottom: 20px; }
    .project-links { margin: 20px 0; }
    .project-links a { display: inline-block; margin: 0 10px; padding: 8px 15px; background-color: #0366d6; color: #fff; text-decoration: none; border-radius: 4px; font-weight: bold; transition: background-color 0.3s; }
    .project-links a:hover { background-color: #0353b3; }
    .project-image { display: block; margin: 0 auto; max-width: 80%; max-height: 400px; object-fit: contain; border-radius: 8px; margin-bottom: 30px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); }

    .project-section { margin-bottom: 40px; }
    .project-section h2 { border-bottom: 2px solid #eaecef; padding-bottom: 10px; color: #24292e; }

    .two-column { display: flex; flex-wrap: wrap; gap: 30px; }
    .column { flex: 1; min-width: 300px; }

    .timeline { position: relative; padding-left: 30px; }
    .timeline-item { margin-bottom: 30px; position: relative; }
    .timeline-item:before { content: ""; position: absolute; left: -30px; top: 5px; width: 15px; height: 15px; border-radius: 50%; background-color: #0366d6; }
    .timeline-item:after { content: ""; position: absolute; left: -23px; top: 20px; bottom: -15px; width: 2px; background-color: #e1e4e8; }
    .timeline-item:last-child:after { display: none; }

    .tech-stack { display: flex; flex-wrap: wrap; gap: 10px; margin-top: 15px; }
    .tech-tag { padding: 5px 10px; background-color: #f1f8ff; border: 1px solid #c8e1ff; border-radius: 3px; font-size: 0.9em; color: #0366d6; }

    .callout { background: #f6f8fa; border-left: 4px solid #0366d6; padding: 15px; border-radius: 4px; }
    .citation { background-color: #f6f8fa; border-left: 4px solid #0366d6; padding: 15px; margin: 20px 0; font-family: monospace; white-space: pre-wrap; }

    .team-members { display: flex; flex-wrap: wrap; gap: 20px; margin-top: 20px; }
    .team-member { text-align: center; width: 140px; }
    .team-member img { width: 90px; height: 90px; border-radius: 50%; object-fit: cover; }
    .team-member p { margin: 6px 0 0 0; font-size: 0.9em; }
  </style>
</head>
<body>
  <div class="navbar">
    <a href="index.html">Home</a>
    <a href="publications.html">Publications</a>
    <a href="service.html">Service</a>
    <a href="talks.html">Talks and Presentations</a>
    <a href="cv.html">Resume/CV</a>
  </div>

  <div class="content">
    <div class="project-header">
      <h1 class="project-title">MFTCXplain: Multilingual Moral Reasoning Benchmark for LLMs</h1>
      <p class="project-subtitle">Evaluating hate-speech <em>multi-hop explanations</em> and moral reasoning across English, Italian, Persian, and Portuguese using expert rationales grounded in Moral Foundations Theory.</p>

      <div class="project-links">
        <a href="MFTCXplain_Paper.pdf" target="_blank">Full Paper (PDF)</a>
        <a href="https://arxiv.org/abs/2506.19073" target="_blank" rel="noopener">arXiv</a>
        <a href="https://github.com/franciellevargas/MFTCXplain" target="_blank" rel="noopener">Dataset & Code</a>
      </div>

      <img src="mftcxplain-figure.png" alt="MFTCXplain overview diagram" class="project-image" />
    </div>

    <div class="project-section">
      <h2>Overview</h2>
      <p><strong>MFTCXplain</strong> tackles one of the central open questions in AI today: 
    <em>How do large language models reason about morality across cultures?</em>
  </p>

  <p>
    Grounded in <strong>social psychology</strong> and <strong>Moral Foundations Theory</strong>, this project brings a social-science lens to AI evaluation.
    We introduce a multilingual benchmark of <strong>3,000 human-annotated tweets</strong> in English, Italian, Persian, and Portuguese—each labeled for 
    hate speech, moral sentiment, and <strong>text-span rationales</strong>. Unlike prior benchmarks, MFTCXplain captures not only <em>what</em> is classified but also 
    <em>why</em>, revealing culturally grounded moral framing patterns across languages.
  </p>

  <p><strong>From a social science perspective, MFTCXplain addresses three challenges:</strong></p>
  <ul>
    <li><strong>Cultural variability in morality:</strong> Hate speech and moral reasoning are deeply moralized but vary by language and cultural context.</li>
    <li><strong>Explanations beyond detection:</strong> While LLMs perform well at hate speech detection, they struggle to capture the underlying moral reasoning—especially in underrepresented languages.</li>
    <li><strong>Transparency &amp; ethical alignment:</strong> By evaluating <em>rationales</em>, we test whether model explanations align with human, culturally grounded moral judgments.</li>
  </ul>

  <p>
    Our findings show that <strong>hate speech rarely occurs without moral sentiment</strong>. 
    Yet even frontier models (e.g., GPT-4o, LLaMA) frequently <strong>misalign with human rationales</strong>, indicating limited capacity to internalize and reflect human moral reasoning across languages.
  </p>

  <p>
    This work reflects my broader PhD focus at USC: <strong>bridging social science, cross-cultural psychology, and AI</strong> to build systems that are 
    <strong>transparent</strong>, <strong>culturally aware</strong>, and <strong>ethically aligned</strong>.
  </p>
       
    <div class="project-section two-column">
      <div class="column">
        <h2>Key Contributions</h2>
        <ul>
          <li><strong>Conceptual design:</strong> Framed research questions using social psychology theory (Moral Foundations Theory), bridging behavioral science with AI evaluation.</li>
    <li><strong>Annotation framework:</strong> Developed guidelines rooted in social and cultural psychology to ensure high-quality, consistent annotations and reliable cross-cultural coding.</li>
    <li><strong>Corpus creation:</strong> Assembled a multilingual dataset (English, Italian, Persian, Portuguese), focusing on underrepresented languages and culturally diverse contexts.</li>
    <li><strong>Data interpretation:</strong> Analyzed findings to uncover cultural variability in moral reasoning, identify gaps in LLM alignment, and translate insights into implications for transparency and ethical AI.</li>
    <li><strong>Collaborative research:</strong> Partnered with computer science collaborators on model benchmarking and technical implementation, ensuring integration of social science and computational approaches.</li>
        </ul>
      </div>
      <div class="column">
        <h2>Research Toolkit</h2
        <div class="tech-stack">
          <span class="tech-tag">Cross-cultural analysis</span>
          <span class="tech-tag">Moral reasoning & ethical alignment</span>
          <span class="tech-tag">Human-centered evaluation</span>
          <span class="tech-tag">Experimental design</span>
          <span class="tech-tag">Annotation guidelines</span>
          <span class="tech-tag">Text analysis</span>
          <span class="tech-tag">LLM prompting</span>
          <span class="tech-tag">Social science + AI integration</span>
        </div>
      </div>
    </div>

    <div class="project-section">
      <h2>Methodology Timeline</h2>
      <div class="timeline">
        <div class="timeline-item">
          <h3>Data Collection</h3>
          <p>Curated <strong>3,000 tweets</strong> spanning four languages from established hate/offense corpora; standardized definitions and included <strong>annotator metadata</strong> to support bias analyses.</p>
        </div>

        <div class="timeline-item">
          <h3>Annotation & Guidelines</h3>
          <p>
            Designed <strong>annotation guidelines</strong> grounded in social psychological theory (Moral Foundations) to label hate, moral sentiment (up to 3 per tweet), and <strong>span-level rationales</strong>. Employed deliberative, annotator-in-the-loop procedures for clarity and quality.
          </p>
        </div>

        <div class="timeline-item">
          <h3>Benchmarking LLMs</h3>
          <p>Evaluated models as <em>objective judges</em> across tasks: hate-speech detection, moral sentiment prediction, and rationale generation under <strong>zero-shot, few-shot, and CoT</strong> prompting.</p>
        </div>

        <div class="timeline-item">
          <h3>Evaluation & Analysis</h3>
          <p>
            Assessed performance via <strong>F1</strong> for hate and moral labels, and <strong>Jaccard/BERTScore</strong> for rationale alignment. Compared languages to surface cultural patterns in moral framing and model weaknesses.
          </p>
        </div>
      </div>
    </div>

    <div class="project-section">
      <h2>Results and Implications</h2>
      <ul>
        <li><strong>Accuracy vs. reasoning gap:</strong> High F1 on hate detection; <strong>substantially lower</strong> for moral sentiment (often &lt; 0.35) and rationale alignment—especially in underrepresented languages.</li>
        <li><strong>Cultural generalization is hard:</strong> Moral framing varies by language, challenging monolingual assumptions and highlighting the need for multilingual resources.</li>
        <li><strong>Applied takeaway:</strong> Progress on safety & integrity tasks requires <strong>explainable, culturally aware</strong> moral reasoning—beyond surface toxicity classification.</li>
      </ul>
    </div>

    <div class="project-section">
      <h2>Publication</h2>
      <p>This project is under active dissemination; paper and resources are publicly available.</p>
      <div class="citation">
@article{MFTCXplain2025,
  title={MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanations},
  year={2025},
  note={Preprint, dataset and code available},
  url={https://arxiv.org/abs/2506.19073}
}
      </div>
    </div>

    <div class="project-section">
      <h2>Team</h2>
      <p>
        I served as the <strong>lead social scientist</strong> on this collaborative project, contributing to <strong>conceptualization</strong>, <strong>corpora assembly</strong>, <strong>annotation guideline creation</strong> (informed by social psychology theory and findings), and <strong>interpretation of results</strong>. I worked closely with <strong>computer science collaborators</strong> who contributed to model development and technical implementation.
      </p>

      <div class="team-members">
        <div class="team-member">
          <img src="profile.jpg" alt="Yalda Daryani"/>
          <p><strong>Yalda Daryani</strong></p>
          <p>Lead Social Scientist</p>
        </div>
        <!-- Add co-authors as desired; placeholders below -->
        <div class="team-member">
          <img src="teammate1.jpg" alt="Collaborator"/>
          <p><strong>Collaborator A</strong></p>
          <p>Computer Science</p>
        </div>
        <div class="team-member">
          <img src="teammate2.jpg" alt="Collaborator"/>
          <p><strong>Collaborator B</strong></p>
          <p>Computer Science</p>
        </div>
      </div>
    </div>

    <div class="project-section">
      <h2>Project Links</h2>
      <p>
        <a href="MFTCXplain_Paper.pdf" target="_blank">Download the paper (PDF)</a> •
        <a href="https://arxiv.org/abs/2506.19073" target="_blank" rel="noopener">arXiv page</a> •
        <a href="https://github.com/franciellevargas/MFTCXplain" target="_blank" rel="noopener">GitHub: Dataset & Code</a>
      </p>
    </div>
  </div>
</body>
</html>
